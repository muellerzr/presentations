{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5c7a97-02d5-4aea-8bd5-59be5e62bf01",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Accelerate, Three Powerful Sublibraries for PyTorch\"\n",
    "author: \"Zachary Mueller\"\n",
    "format: \n",
    "    revealjs:\n",
    "        theme: moon\n",
    "        fig-format: png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e61402-f734-4500-8eb6-fcdd6f17a0d4",
   "metadata": {},
   "source": [
    "## Who am I?\n",
    "\n",
    "- Zachary Mueller\n",
    "- Deep Learning Software Engineer at ðŸ¤—\n",
    "- API design geek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9864d2-5787-4af3-a08d-b372e5851a0f",
   "metadata": {},
   "source": [
    "## What is ðŸ¤— Accelerate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b148a-e2f0-46b0-bc61-ac6e81da5ac5",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%| fig-height: 6\n",
    "graph LR\n",
    "    A{\"ðŸ¤— Accelerate#32;\"}\n",
    "    A --> B[\"Launching<br>Interface#32;\"]\n",
    "    A --> C[\"Training Library#32;\"]\n",
    "    A --> D[\"Big Model<br>Inference#32;\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6fd12-18cd-4448-9123-821133673b95",
   "metadata": {},
   "source": [
    "# A Launching Interface\n",
    "\n",
    "Can't I just use `python do_the_thing.py`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5488645-daa3-4353-be9f-7af765a52666",
   "metadata": {},
   "source": [
    "## A Launching Interface\n",
    "\n",
    "Launching scripts in different environments is complicated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce856633-1909-4f18-9610-e934194dd584",
   "metadata": {},
   "source": [
    "- ```bash \n",
    "python script.py\n",
    "```\n",
    "\n",
    "- ```bash \n",
    "torchrun --nnodes=1 --nproc_per_node=2 script.py\n",
    "```\n",
    "\n",
    "- ```bash \n",
    "deepspeed --num_gpus=2 script.py\n",
    "```\n",
    "\n",
    "And more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6414d0-f8f8-4bd2-b06f-fe7f848320f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A Launching Interface\n",
    "\n",
    "But it doesn't have to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd30c0-7240-4a13-9b51-061c4762b37e",
   "metadata": {},
   "source": [
    "```bash\n",
    "accelerate launch script.py\n",
    "```\n",
    "\n",
    "A single command to launch with `DeepSpeed`, Fully Sharded Data Parallelism, across single and multi CPUs and GPUs, and to train on TPUs[^1] too! \n",
    "\n",
    "[^1]: Without needing to modify your code and create a `_mp_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0760c9a-4307-4143-9adc-bf1ce2ed4460",
   "metadata": {},
   "source": [
    "## A Launching Interface\n",
    "\n",
    "Generate a device-specific configuration through `accelerate config`\n",
    "\n",
    "![](CLI.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1dc7a-ec43-48ba-b0a0-1331981733d0",
   "metadata": {},
   "source": [
    "## A Launching Interface\n",
    "\n",
    "Or don't. `accelerate config` doesn't *have* to be done!\n",
    "\n",
    "```bash\n",
    "torchrun --nnodes=1 --nproc_per_node=2 script.py\n",
    "accelerate launch --multi_gpu --nproc_per_node=2 script.py\n",
    "```\n",
    "\n",
    "A quick default configuration can be made too:\n",
    "\n",
    "```bash \n",
    "accelerate config default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d2c3d-5a08-4e5b-9896-1a0bcb77b5a6",
   "metadata": {},
   "source": [
    "## A Launching Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a395af44-96f8-4f3a-ac47-3f65a6062d24",
   "metadata": {},
   "source": [
    "With the `notebook_launcher` it's also possible to launch code directly from your Jupyter environment too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b14b46-6be5-4ef4-a3ee-82876b1d7802",
   "metadata": {},
   "source": [
    "```python\n",
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(\n",
    "    training_loop_function, \n",
    "    args, \n",
    "    num_processes=2\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e27a7-4235-4695-bf99-59c0f3d0e451",
   "metadata": {},
   "source": [
    "```python\n",
    "Launching training on 2 GPUs.\n",
    "epoch 0: 88.12\n",
    "epoch 1: 91.73\n",
    "epoch 2: 92.58\n",
    "epoch 3: 93.90\n",
    "epoch 4: 94.71\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4e66d-d8b0-4f3f-9236-e86c1c3ea5d2",
   "metadata": {},
   "source": [
    "# A Training Library\n",
    "\n",
    "Okay, will `accelerate launch` make `do_the_thing.py` use all my GPUs magically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd093ef-d3ce-4ea4-89a1-be145fbe5cc0",
   "metadata": {},
   "source": [
    "## A Training Library\n",
    "\n",
    "- Just showed that its possible using `accelerate launch` to *launch* a python script in various distributed environments\n",
    "- This does *not* mean that the script will just \"use\" that code and still run on the new compute efficiently.\n",
    "- Training on different computes often means *many* lines of code changed for each specific compute.\n",
    "- ðŸ¤— `accelerate` solves this by ensuring the same code can be ran on a CPU or GPU, multiples, and on TPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b12eb9-feeb-4040-a784-8e78966165be",
   "metadata": {},
   "source": [
    "## A Training Library\n",
    "\n",
    "\n",
    "```{.python}\n",
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb72602-f86f-42f6-ab44-05fbd0dfcecd",
   "metadata": {},
   "source": [
    "## A Training Library {.smaller}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f90b84-fff5-4c14-bde7-d1efbcc37781",
   "metadata": {},
   "source": [
    ":::: {.columns}\n",
    "::: {.column width=\"43%\"}\n",
    "<br><br><br>\n",
    "```{.python code-line-numbers=\"5-6,9\"}\n",
    "# For alignment purposes\n",
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "```\n",
    ":::\n",
    "::: {.column width=\"57%\"}\n",
    "```{.python code-line-numbers=\"1-7,12-13,16\"}\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "dataloader, model, optimizer scheduler = (\n",
    "    accelerator.prepare(\n",
    "        dataloader, model, optimizer, scheduler\n",
    "    )\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    # inputs = inputs.to(device)\n",
    "    # targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    accelerator.step(loss) # optimizer.step()\n",
    "    scheduler.step()\n",
    "```\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c90913-2542-4b1d-8121-b2228c8a2ef7",
   "metadata": {},
   "source": [
    "## A Training Library\n",
    "\n",
    "What all happened in `Accelerator.prepare`?\n",
    "\n",
    "::: {.incremental}\n",
    "1. `Accelerator` looked at the configuration\n",
    "2. The `training_dataloader` was converted into one that can dispatch each batch onto a seperate GPU\n",
    "3. The `model` was wrapped with the appropriate DDP wrapper from either `torch.distributed` or `torch_xla`\n",
    "4. The `optimizer` and `scheduler` were both converted into an `AcceleratedOptimizer` and `AcceleratedScheduler` which knows how to handle any distributed scenario\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7008a8-1ff2-4f16-aab1-5e0313f44de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
